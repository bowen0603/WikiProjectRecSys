from __future__ import print_function
from page_parser import PageParser
from wikitable_generator import TableGenerator
from recommend_cf_editors import UUCF


__author__ = 'bobo'

"""
Related APIs for different methods
1. identify_newcomers_and_experienced_editors(): https://www.mediawiki.org/w/api.php?action=help&modules=query%2Busers
TODO: need to handle maximum request threshold and sleeping...

2. Create active editors in the past week to start with:
    https://quarry.wmflabs.org/query/20246


3. Files to be updated:

 Files the system will collect or read from generated file
(1) project_pages: collect the pages within the scope of the project
(2) project_members.csv: the contributors of the project
(3) valid_experienced_editors.csv: editors who made 100+ edits and were valid
(4) editor_validation.csv: editors who are not blocked or vandal

 Files uploaded outside the system
  (1) active_editors_past_week.csv: (Quarry) active editors in the last week
  (2) **article_categories.csv: (Bigquery) the cateogries of the articles
  (3) bot_list.csv
  (4) projects_categories.csv: (Bigquery) generated by XXX.py
  (5) Top25ProjectsPastYear.csv: project list
  (6) article_top25_projects.csv: articles within the scope of the sample projects

  TODOs:
  1. figure out the issue with the current running time.
  2. add blocked editors; make it a separated loop outside
  3. extract the info needed to explain the recommendations for each algorithm
  4. handle data reuse issue
  5. final output
  6. user-user cf issue
"""
import requests
from time import sleep
import random
from datetime import datetime
import os.path
from random import shuffle
import math


class RecommendExperienced():
    def __init__(self, argv):

        # TODO to chagne
        self.const_one_month_in_days = 30
        self.threshold_very_active = 100
        self.threshold_active = 5
        self.const_very_active = "Very Active"
        self.const_active = "Active"

        self.const_recommendation_nbr = 40  # 20
        self.const_max_requests = 500  # 500
        self.constr_newcomer_days = 7
        self.debug = False
        self.debug_nbr = 100

        self.list_active_editors = []
        self.list_bots = []
        self.list_sample_projects = []

        self.dict_exp_editor_text_id = {}
        self.dict_editor_text_editcount = {}
        self.dict_editor_last_edit_datetime = {}
        # computed by the contributors of project related pages
        # self.dict_project_members = {}
        self.dict_project_sub_pages = {}
        self.dict_project_sub_talkpages = {}
        self.dict_page_title_id = {}
        self.dict_project_contributors = {}
        self.dict_member_article_edited = {}

        self.set_valid_newcomers = set()
        self.set_valid_exp_editors = set()

        self.dict_newcomer_text_id = {}
        self.dict_editor_regstr_time = {}
        self.dict_newcomer_editcount = {}
        self.dict_newcomer_first_edit_article = {}
        self.dict_editor_project_talker_nbr = {}
        self.dict_topic_editor_first_category = {}
        self.dict_topic_editor_second_category = {}
        self.dict_editor_status = {}

        self.exp_editor_thr = 100

        self.url_userinfo = "https://en.wikipedia.org/w/api.php?action=query&format=json&list=users"
        self.url_usercontb = "https://en.wikipedia.org/w/api.php?action=query&format=json&list=usercontribs&"
        self.url_propages = "https://en.wikipedia.org/w/api.php?action=query&format=json&list=prefixsearch&"
        self.url_contributors = "https://en.wikipedia.org/w/api.php?action=query&format=json&prop=contributors&"

        self.list_active_editors = self.read_active_editors()
        self.list_sample_projects, self.dict_topic_based_recommendation, self.dict_rule_based_recommendation, \
            self.dict_bonds_based_recommendation, self.dict_uucf_based_recommendation, \
            self.dict_project_newcomer_edits, self.dict_uucf_similar_project_member, self.dict_uucf_common_edits = self.read_sample_projects()
        self.list_bots = self.read_bot_list()

        self.page_parser = PageParser()
        # self.table_generator = TableGenerator()

        # the projects an article within the scope of - parsed from article talk pages
        self.dict_article_projects = self.read_article_projects()
        self.dict_article_categories = self.read_article_categories()

        self.dict_project_categories = self.read_project_categories()


    def identify_valid_newcomers_and_experienced_editors(self):
        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/valid_experienced_editors.csv"

        if os.path.isfile(fname):
            self.read_valid_experienced_editors_from_file()
            self.read_valid_newcomer_from_file()
        else:

            self.identify_newcomers_and_experienced_editors()
            self.remove_blocked_or_vandal_editors()

            self.write_valid_experienced_editors_to_file()
            self.write_valid_newcomers_to_file()


    def remove_blocked_or_vandal_editors(self):
        print('### Removing blocked or vandal newcomers and experienced editors ###')
        cnt_editor, set_editors = 0, set()
        list_editors = list(self.dict_newcomer_text_id.keys())
        for i in range(len(list_editors)):
            editor_text = list_editors[i]
            if cnt_editor < 45 and i != len(list_editors)-1:
                cnt_editor += 1
                set_editors.add(editor_text)
            else:
                set_editors.add(editor_text)
                valid_editors = self.page_parser.check_editors_validation(set_editors)
                for valid_editor in valid_editors.keys():
                    if valid_editors[valid_editor]:
                        self.set_valid_newcomers.add(valid_editor)
                cnt_editor, set_editors = 0, set()

        cnt_editor, set_editors = 0, set()
        list_editors = list(self.dict_exp_editor_text_id.keys())
        for i in range(len(list_editors)):
            editor_text = list_editors[i]
            if cnt_editor < 45 and i != len(list_editors) - 1:
                cnt_editor += 1
                set_editors.add(editor_text)
            else:
                set_editors.add(editor_text)
                valid_editors = self.page_parser.check_editors_validation(set_editors)
                for valid_editor in valid_editors.keys():
                    if valid_editors[valid_editor]:
                        self.set_valid_exp_editors.add(valid_editor)
                cnt_editor, set_editors = 0, set()
        print("#### {} valid newcomers, and {} valid experienced editors".format(len(self.set_valid_newcomers),
                                                                                 len(self.set_valid_exp_editors)))


    def identify_experienced_editors(self, set_editors):
        cnt_editor, str_editors, set_exp_edtiors = 0, "", set()
        list_editors = list(set_editors)
        for i in range(len(list_editors)):
            editor_text = list_editors[i]
            if cnt_editor < 45 and i != len(list_editors)-1:
                cnt_editor += 1
                str_editors += editor_text + "|"
            else:
                try:
                    str_editors += editor_text + "|"
                    query = self.url_userinfo + "&usprop=editcount&ususers=" + str_editors
                    response = requests.get(query).json()
                    for editor_info in response['query']['users']:
                        if 'userid' not in editor_info:
                            continue

                        editor_text = editor_info['name']
                        editor_id = editor_info['userid']
                        editor_editcount = editor_info['editcount']

                        if editor_editcount >= self.exp_editor_thr and editor_text not in self.list_bots:
                            set_exp_edtiors.add(editor_text)

                except KeyError:
                    if self.catch_error_to_sleep(response):
                        continue
                    else:
                        break
                except requests.exceptions.ConnectionError:
                    print("Max retries exceeded with url.")
                    sleep(5)
                    continue
                except:
                    print("Throwing except: {}".format(response))
                    continue

                cnt_editor, str_editors = 0, ""
        print("Number of experienced contributors: {}".format(len(set_exp_edtiors)))
        return set_exp_edtiors


    # query the active editors to obtain their total edits in Wikipedia
    def identify_newcomers_and_experienced_editors(self):
        print("### Identifying newcomers and active experienced editors to recommend ###")

        cnt_total_editor, cnt_editor, str_editors = 0, 0, ""
        cnt_none_registration_date = 0
        for i in range(len(self.list_active_editors)):
            editor_text = self.list_active_editors[i]

            if self.debug:
                if cnt_total_editor > self.debug_nbr:
                    break
                cnt_total_editor += 1

            # create a list of editors to request at the same time (50 maximum)
            if cnt_editor < 45 and i != len(self.list_active_editors)-1:
                cnt_editor += 1
                str_editors += editor_text + "|"
            else:
                try:
                    str_editors += editor_text + "|"
                    query = self.url_userinfo + "&usprop=editcount|registration&ususers=" + str_editors
                    response = requests.get(query).json()
                    for editor_info in response['query']['users']:
                        if 'userid' not in editor_info:
                            continue

                        editor_text = editor_info['name']
                        editor_id = editor_info['userid']
                        editor_editcount = editor_info['editcount']
                        editor_regstr_ts = editor_info['registration']

                        if editor_regstr_ts is None:
                            delta_days = self.constr_newcomer_days + 1
                            cnt_none_registration_date += 1
                            continue
                        else:
                            regstr_datetime = datetime.strptime(editor_regstr_ts, "%Y-%m-%dT%H:%M:%SZ")
                            delta_days = (datetime.now() - regstr_datetime).days

                        self.dict_editor_regstr_time[editor_text] = regstr_datetime

                        # collect data for newcomers
                        if delta_days <= self.constr_newcomer_days and editor_text not in self.list_bots:
                            self.dict_newcomer_text_id[editor_text] = editor_id
                            self.dict_newcomer_editcount[editor_text] = editor_editcount

                        # collect data for experienced editors
                        if editor_editcount >= self.exp_editor_thr and editor_text not in self.list_bots:
                            self.dict_exp_editor_text_id[editor_text] = editor_id
                            self.dict_editor_text_editcount[editor_text] = editor_editcount

                except KeyError:
                    if self.catch_error_to_sleep(response):
                        continue
                    else:
                        break
                except requests.exceptions.ConnectionError:
                    print("Max retries exceeded with url.")
                    sleep(5)
                    continue
                except:
                    print("Throwing except: {}".format(response))
                    continue

                    # print("{},{},{}".format(editor_text, editor_id, editor_editcount))

                cnt_editor, str_editors = 0, ""
        print("Number of active editors: {}; experienced editors: {}; newcomers: {}".format(len(self.list_active_editors),
                                                                                            len(self.dict_exp_editor_text_id),
                                                                                            len(self.dict_newcomer_text_id)))

        print("##NONE registration date: {}".format(cnt_none_registration_date))

    def recommend_editors(self):
        print("#### Working on {} newcomers... ####".format(len(self.set_valid_newcomers)))
        self.fetch_history(self.set_valid_newcomers, True)
        self.write_newcomer_recommendations()

        print("#### Working on {} experienced editors... ####".format(len(self.set_valid_exp_editors)))
        self.fetch_history(self.set_valid_exp_editors, False)

        self.write_rule_recommendations()
        self.write_bonds_recommendations()
        self.write_topic_recommendations()
        self.write_uucf_recommendations()


    def fetch_history(self, editor_list, is_newcomers):

        # TODO: write members that are done fetching into a file with the latest timestamp
        # TODO: write the articles editor edited into files to fasten the process
        editor_cnt = 0

        for editor_text in editor_list:
            if editor_text == '':
                continue

            if editor_cnt % 1000 == 0:
                print("## Retrieving and analyzing {}k editors.. ".format(editor_cnt / 1000))

            # print("#{}. Retrieving and analyzing edits of editor: {}.".format(editor_cnt, editor_text))
            editor_cnt += 1
            cnt_mainpage_edits = 0
            latest_datetime = datetime.fromordinal(1)
            current_datetime = datetime.now()

            edits_ns0_articles = {}
            projects_within_30days = {}
            edits_ns3_users = {}

            # Just fetch 500 edits using one query for each editor
            try:

                query = self.url_usercontb + "uclimit=" + str(self.const_max_requests) + "&ucprop=title|timestamp|parsedcomment|flags|ids&ucuser=" + editor_text
                response = requests.get(query).json()


                for usercontrib in response['query']['usercontribs']:

                    page_title = usercontrib['title']
                    page_id = usercontrib['pageid']
                    ns = usercontrib['ns']
                    userid = usercontrib['userid']

                    edit_datetime = datetime.strptime(usercontrib['timestamp'], "%Y-%m-%dT%H:%M:%SZ")
                    latest_datetime = max(edit_datetime, latest_datetime)
                    self.dict_editor_last_edit_datetime[editor_text] = latest_datetime

                    if 'parsedcomment' in usercontrib:
                        comment = usercontrib['parsedcomment']
                        if comment.startswith('Revert'):
                            continue
                        # if usercontrib['flags'] == 'minor':
                        #     continue

                        if 'flags' in usercontrib:
                            print(usercontrib['flags'])
                            continue

                    if ns == 0:
                        if is_newcomers:
                            if editor_text not in self.dict_newcomer_first_edit_article:
                                self.dict_newcomer_first_edit_article[editor_text] = page_title
                            else:
                                # keep update until the last record which is the first edit of the editor
                                self.dict_newcomer_first_edit_article[editor_text] = page_title

                        page_title = page_title.lower()
                        edits_ns0_articles[page_title] = 1 if page_title not in edits_ns0_articles \
                            else edits_ns0_articles[page_title] + 1

                        # count the number of edits within 30 days
                        if (current_datetime - edit_datetime).days <= self.const_one_month_in_days:
                            cnt_mainpage_edits += 1

                            # identify active projects within 30 days.
                            if page_title in self.dict_article_projects:
                                for project in self.dict_article_projects[page_title]:
                                    if project in projects_within_30days:
                                        projects_within_30days[project] += 1
                                    else:
                                        projects_within_30days[project] = 1

                    elif ns == 3:
                        if not is_newcomers:
                            edits_ns3_users[page_title] = 1 if page_title not in edits_ns3_users \
                                else edits_ns3_users[page_title] + 1
                    else:
                        # don't really need to do anything for edits on project pages,
                        # since project contributors have been identified
                        pass
            # except :
            #     import sys
            #     e = sys.exc_info()[0]
            #     print("Error: {}".format(e))
            except KeyError:
                if self.catch_error_to_sleep(response):
                    continue
                else:
                    break
            except requests.exceptions.ConnectionError:
                print("Max retries exceeded with url.")
                sleep(5)
                continue
            except:
                print("Throwing except: {}".format(response))
                continue

            # only select active projects for further recommendations
            list_candidate_projects = []
            for project in projects_within_30days:
                if projects_within_30days[project] >= 2:
                    list_candidate_projects.append(project)

            # process editor's revisions for recommendations

            # handle rule-based recommendation by article page edits
            stats_edits_projects_articles = self.compute_project_article_edits(edits_ns0_articles)
            if is_newcomers:
                self.maintain_project_newcomer_recommendation_lists(editor_text, stats_edits_projects_articles)
            else:

                if cnt_mainpage_edits >= self.threshold_very_active:
                    self.dict_editor_status[editor_text] = self.const_very_active
                elif cnt_mainpage_edits >= self.threshold_active:
                    self.dict_editor_status[editor_text] = self.const_active
                else:
                    continue

                editor_topic_vector = self.compute_editor_topic_vector(edits_ns0_articles)
                # ignore not valid editors
                if not editor_topic_vector:
                    continue

                self.maintain_project_topic_based_recommendation_lists(editor_text, editor_topic_vector, list_candidate_projects)

                self.maintain_project_rule_based_recommendation_lists(editor_text, stats_edits_projects_articles, list_candidate_projects)

                stats_edits_projects_users, self.dict_editor_project_talker_nbr[editor_text] = \
                    self.compute_project_user_edits(edits_ns3_users)
                self.maintain_project_bonds_based_recommendation_lists(editor_text, stats_edits_projects_users, list_candidate_projects)

                self.main_project_uucf_recommendation_list(editor_text, edits_ns0_articles, list_candidate_projects)


    def maintain_project_topic_based_recommendation_lists(self, user_text, editor_topic_vector, list_candidate_projects):
        for project in self.dict_project_categories:
            if project not in self.list_sample_projects:
                continue

            if project not in list_candidate_projects:
                continue

            if project in ['Women', 'Women artists', 'Women\'s History', 'Women scientists']:
                continue

            val = self.compute_topic_cosine_similarity(editor_topic_vector, self.dict_project_categories[project])

            # todo: how to deal with the duplicated recommendations for different projects ??

            # insert the new editor
            dict_recommended_editors = self.dict_topic_based_recommendation[project]
            dict_recommended_editors[user_text] = val

            value_max = -1
            category_max = None
            for category in editor_topic_vector:
                if category == 'Not Found':
                    continue
                if editor_topic_vector[category] > value_max:
                    value_max = editor_topic_vector[category]
                    category_max = category
            self.dict_topic_editor_first_category[user_text] = category_max

            value_max = -1
            category_sec_max = None
            for category in editor_topic_vector:
                if category == 'Not Found':
                    continue
                if editor_topic_vector[category] > value_max and category != category_max:
                    value_max = editor_topic_vector[category]
                    category_sec_max = category

            # if category_sec_max == None:
            #     print("{},{}".format(user_text, self.dict_editor_text_editcount[user_text]))
            self.dict_topic_editor_second_category[user_text] = category_sec_max

            # identify the least to recommending editor, and remove it from the list
            if len(dict_recommended_editors) > self.const_recommendation_nbr:
                editor_min = min(dict_recommended_editors, key=dict_recommended_editors.get)
                del dict_recommended_editors[editor_min]
                # del self.dict_topic_editor_first_category[editor_min]
                # del self.dict_topic_editor_second_category[editor_min]

    # maintain the list in sorted order by editors who made top N edits on artiles within the scope of the project
    def maintain_project_rule_based_recommendation_lists(self, user_text, stats_edits_projects_articles, list_candidate_projects):
        for project in self.dict_project_contributors.keys():

            if project not in self.list_sample_projects:
                continue

            if project not in list_candidate_projects:
                continue

            # skip if is project member
            if project in self.dict_project_contributors and user_text in self.dict_project_contributors[project]:
                continue

            if project not in stats_edits_projects_articles:
                continue

            # insert the new editor
            dict_recommended_editors = self.dict_rule_based_recommendation[project]
            dict_recommended_editors[user_text] = stats_edits_projects_articles[project]

            # identify the least to recommending editor, and remove it from the list
            if len(dict_recommended_editors) > self.const_recommendation_nbr:
                editor_min = min(dict_recommended_editors, key=dict_recommended_editors.get)
                del dict_recommended_editors[editor_min]


    def maintain_project_bonds_based_recommendation_lists(self, user_text, stats_edits_projects_users, list_candidate_projects):
        for project in self.dict_project_contributors:

            if project not in self.list_sample_projects:
                continue

            if project not in list_candidate_projects:
                continue

            # skip if is project member
            if project in self.dict_project_contributors and user_text in self.dict_project_contributors[project]:
                continue

            if project not in stats_edits_projects_users:
                continue

            # insert the new editor
            dict_recommended_editors = self.dict_bonds_based_recommendation[project]
            dict_recommended_editors[user_text] = stats_edits_projects_users[project]

            if len(dict_recommended_editors) > self.const_recommendation_nbr:
                editor_min = min(dict_recommended_editors, key=dict_recommended_editors.get)
                del dict_recommended_editors[editor_min]


    def main_project_uucf_recommendation_list(self, editor_text, edits_ns0_articles, list_candidate_projects):
        for project in self.dict_project_contributors:

            if project not in self.list_sample_projects:
                continue

            if project not in list_candidate_projects:
                continue

            # skip if is project member
            if project in self.dict_project_contributors and editor_text in self.dict_project_contributors[project]:
                continue


            total_score = 0
            max_score = -1

            for member in self.dict_project_contributors[project]:
                if member in self.list_bots:
                    continue

                set_article_member = self.dict_member_article_edited[member]
                set_article_editor = set(edits_ns0_articles.keys())
                article_shared = set_article_member & set_article_editor

                # insersect(A,B)/(sqrt(len(A))*sqrt(len(B)))
                uucf_score = 1.0 * len(article_shared) / (math.sqrt(len(set_article_member)) * math.sqrt(len(set_article_editor)))
                total_score += uucf_score
                if uucf_score > max_score:
                    max_score = uucf_score
                    self.dict_uucf_similar_project_member[project][editor_text] = member
                    self.dict_uucf_common_edits[project][editor_text] = len(article_shared)


            # insert the new editor
            dict_recommended_editors = self.dict_uucf_based_recommendation[project]
            dict_recommended_editors[user_text] = total_score

            if len(dict_recommended_editors) > self.const_recommendation_nbr:
                editor_min = min(dict_recommended_editors, key=dict_recommended_editors.get)
                del dict_recommended_editors[editor_min]

    # randomly pick one project for the newcomer to recommend based on the first article the editor edited
    def maintain_project_newcomer_recommendation_lists(self, editor_text, stats_edits_projects_articles):
        # it is possible that the new editor didn't edit the article page
        if editor_text not in self.dict_newcomer_first_edit_article:
            return

        if self.dict_newcomer_first_edit_article[editor_text].lower() in self.dict_article_projects:
            # TODO: to keep it updated, we need to have updated the article-project file, making sure all the projects are within the scope
            projects = self.dict_article_projects[self.dict_newcomer_first_edit_article[editor_text].lower()]
            project = random.choice(projects)

            project_members = self.dict_project_newcomer_edits[project]
            # get the edits within the project
            project_members[editor_text] = stats_edits_projects_articles[project]


    def compute_editor_topic_vector(self, edits_article_pages):
        editor_topic_vector = {}
        for article in edits_article_pages:
            cnt_edits = edits_article_pages[article]

            # get the project of the articles read from dump data
            if article in self.dict_article_categories:
                categories = self.dict_article_categories[article]

                # nested structure with both (category, category weight)
                for category in categories:
                    weight = cnt_edits * categories[category]
                    editor_topic_vector[category] = weight if category not in editor_topic_vector \
                        else editor_topic_vector[category] + weight
        sum_value = sum(editor_topic_vector.values())
        if sum_value != 0:
            factor = 1.0 / sum_value
            editor_topic_vector = {k: v * factor for k, v in editor_topic_vector.items()}
        return editor_topic_vector

    # computer article edits on the sample projects
    def compute_project_article_edits(self, edits_article_pages):

        stats_edits_project_articles = {}
        for article in edits_article_pages:
            cnt_edits = edits_article_pages[article]

            # get the project of the articles read from dump data
            if article in self.dict_article_projects:
                projects = self.dict_article_projects[article]

                for project in projects:
                    stats_edits_project_articles[project] = cnt_edits if project not in stats_edits_project_articles \
                        else stats_edits_project_articles[project] + cnt_edits
                    # hard code for WikiProject Women in Red
                    if project in ['Women', 'Women artists', 'Women\'s History', 'Women scientists']:
                        stats_edits_project_articles['Women in Red'] = cnt_edits if 'Women in Red' not in stats_edits_project_articles \
                            else stats_edits_project_articles['Women in Red'] + cnt_edits
        return stats_edits_project_articles

    # handle user talk pages (ns 3)
    def compute_project_user_edits(self, edits_user_pages):
        stats_edits_users = {}
        stats_project_per_talker = {}
        for page in edits_user_pages:
            cnt_edits = edits_user_pages[page]
            user_text = page.replace("User talk:", "")

            for project in self.list_sample_projects:
                if project in self.dict_project_contributors and \
                                user_text not in self.dict_project_contributors[project]:
                    continue
                stats_edits_users[project] = cnt_edits if project not in stats_edits_users \
                    else stats_edits_users[project] + cnt_edits
                stats_project_per_talker[project] = 1 if project not in stats_project_per_talker \
                    else stats_project_per_talker[project] + 1
        return stats_edits_users, stats_project_per_talker

    # handle project talk pages (ns 4 and 5)
    def compute_project_page_edits(self, edits_project_pages):
        stats_edits_projects = {}
        for page in edits_project_pages:
            cnt_edits = edits_project_pages[page]

            for project in self.list_sample_projects:
                if project in self.dict_project_contributors and page not in self.dict_project_sub_talkpages[project]:
                    continue

                stats_edits_projects[project] = cnt_edits if project not in stats_edits_projects \
                    else stats_edits_projects[project] + cnt_edits

        return stats_edits_projects

    def collect_project_related_pages(self):
        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/project_pages.csv"

        if os.path.isfile(fname):
            print("### Reading related pages of WikiProjects from file ###")
            for line in open(fname, 'r'):
                project = line.split("*")[0].strip()
                page = line.split('*')[1].strip()
                if project in self.dict_project_sub_pages:
                    self.dict_project_sub_pages[project].append(page)
                else:
                    self.dict_project_sub_pages[project] = [page]
            for project in self.dict_project_sub_pages.keys():
                print("Reading {} related pages for WikiProject: {}.".format(len(self.dict_project_sub_pages[project]),
                                                                             project))
        else:
            print("### Collecting related pages of WikiProjects, and writing into file ###")
            fout = open(fname, 'w')
            for project in self.list_sample_projects:
                search_name = "Wikipedia:WikiProject " + project
                set_project_pages = self.search_project_pages(search_name)
                self.dict_project_sub_pages[project] = list(set_project_pages)

                search_name = "Wikipedia talk:WikiProject " + project
                set_project_talk_pages = self.search_project_pages(search_name)
                self.dict_project_sub_talkpages[project] = list(set_project_talk_pages)

                print("Collected pages for WikiProject:{}. {} related pages.".format(project,
                                                                                     len(set_project_pages) +
                                                                                     len(set_project_talk_pages)))
                # write into file
                for page in set_project_pages:
                    print("{}*{}".format(project, page), file=fout)
                for page in set_project_talk_pages:
                    print("{}*{}".format(project, page), file=fout)
        print()


    def identify_project_members_and_edits(self):

        # read into a list; create into a set
        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/project_experienced_contributors.csv"
        if os.path.isfile(fname):
            self.read_experienced_project_contributors()
        else:
            print("### Collecting members of WikiProjects, and writing into file ###")
            for project in self.list_sample_projects:
                contributors = set()
                experienced_contributors = set()
                if project in self.dict_project_sub_pages:

                    shuffle(self.dict_project_sub_pages[project])
                    page_contributors = self.search_page_contributors(self.dict_project_sub_pages[project])
                    experienced_contributors = experienced_contributors.union(self.identify_experienced_editors(page_contributors))
                    # print("exp: {}, total contributors: {}".format(len(experienced_contributors), len(page_contributors)))


                if project in self.dict_project_sub_talkpages:
                    shuffle(self.dict_project_sub_talkpages[project])
                    page_contributors = self.search_page_contributors(self.dict_project_sub_talkpages[project])
                    experienced_contributors = experienced_contributors.union(self.identify_experienced_editors(page_contributors))
                    # print("exp: {}, total contributors: {}".format(len(experienced_contributors), len(page_contributors)))

                self.dict_project_contributors[project] = experienced_contributors
                print("Collecting contributors for WikiProject:{}. {} contributors.".format(project,
                                                                                            len(experienced_contributors)))
                # # write into files
                # for contributor in experienced_contributors:
                #     print("{}*{}".format(project, contributor), file=fout)

            # run it again for duplicated ones
            for project in self.list_sample_projects:
                contributors = set()
                experienced_contributors = set()
                if project in self.dict_project_sub_pages:
                    shuffle(self.dict_project_sub_pages[project])
                    page_contributors = self.search_page_contributors(self.dict_project_sub_pages[project])
                    experienced_contributors = experienced_contributors.union(self.identify_experienced_editors(page_contributors))
                    # print("exp: {}, total contributors: {}".format(len(experienced_contributors), len(page_contributors)))

                if project in self.dict_project_sub_talkpages:
                    page_contributors = self.search_page_contributors(self.dict_project_sub_talkpages[project])
                    experienced_contributors = experienced_contributors.union(self.identify_experienced_editors(page_contributors))
                    # print("exp: {}, total contributors: {}".format(len(experienced_contributors), len(page_contributors)))

                self.dict_project_contributors[project] = self.dict_project_contributors[project].union(experienced_contributors)
                print("Collecting contributors for WikiProject:{}. {} contributors.".format(project,
                                                                                            len(experienced_contributors)))

                print("Unioned number for project: {}, {}".format(project, len(self.dict_project_contributors[project])))

            self.write_valid_project_contributors_to_file()
        # todo: for project members, collect articles in their most recent 500 edits
        self.collect_member_article_edits()
        print()

    def collect_member_article_edits(self):

        print("### Collecting member article edits ###")
        for project in self.list_sample_projects:
            if project not in self.dict_project_contributors:
                continue
            cnt = 0
            for member in self.dict_project_contributors[project]:

                #TODO: remove
                cnt += 1
                if cnt >= 50:
                    break

                if member == '':
                    continue

                # print("#{}. Retrieving and analyzing edits of editor: {}.".format(editor_cnt, editor_text))
                set_article_pages = set()

                cnt_mainpage_edits = 0
                latest_datetime = datetime.fromordinal(1)
                current_datetime = datetime.now()

                edits_ns0_articles = {}
                projects_within_30days = {}
                edits_ns3_users = {}

                # Just fetch 500 edits using one query for each editor
                try:

                    query = self.url_usercontb + "uclimit=" + str(
                        self.const_max_requests) + "&ucprop=title|timestamp|parsedcomment|flags|ids&ucuser=" + member
                    response = requests.get(query).json()

                    for usercontrib in response['query']['usercontribs']:

                        page_title = usercontrib['title']
                        ns = usercontrib['ns']

                        if 'parsedcomment' in usercontrib:
                            comment = usercontrib['parsedcomment']
                            if comment.startswith('Revert'):
                                continue
                            # if usercontrib['flags'] == 'minor':
                            #     continue

                            if 'flags' in usercontrib:
                                print(usercontrib['flags'])
                                continue

                        if ns == 0:
                            page_title = page_title.lower()
                            set_article_pages.add(page_title)

                # except :
                #     import sys
                #     e = sys.exc_info()[0]
                #     print("Error: {}".format(e))
                except KeyError:
                    if self.catch_error_to_sleep(response):
                        continue
                    else:
                        break
                except requests.exceptions.ConnectionError:
                    print("Max retries exceeded with url.")
                    sleep(5)
                    continue
                except:
                    print("Throwing except: {}".format(response))
                    continue

                self.dict_member_article_edited[member] = set_article_pages



    def constr_original_page(self, pages):
        query = self.url_contributors + "pclimit=" + str(self.const_max_requests) + "&titles=" + pages
        return query

    def constr_next_page(self, pages, cont):
        query = self.url_contributors + "pclimit=" + str(
            self.const_max_requests) + "&pccontinue=" + cont + "&titles=" + pages
        return query

    def search_page_contributors(self, page_titles):

        cnt_page, str_pages, pccontinue = 0, "", ""
        contributors = set()
        page_set = set()

        # create a list of pages to request at the same time (50 maximum)
        for page_title in set(page_titles):
            if cnt_page < 40:
                cnt_page += 1
                if page_title not in page_set:
                    str_pages += page_title + "|"
                    page_set.add(page_title)
            else:
                first_request, continue_querying = True, True
                while continue_querying:
                    try:
                        if first_request:
                            query = self.constr_original_page(str_pages)
                            first_request = False
                        else:
                            query = self.constr_next_page(str_pages, pccontinue)

                        # query = self.url_contributors + "pclimit="+str(self.const_max_requests)+"&pccontinue=&titles=" + page_title
                        response = requests.get(query).json()
                        if 'continue' in response:
                            pccontinue = response['continue']['pccontinue']
                        else:
                            continue_querying = False

                        # a bit complicated nested data structure in response, but here only to extract page contributors
                        pages_object = response['query']['pages']
                        for page in pages_object.keys():
                            if 'contributors' in pages_object[page]:
                                for editor in pages_object[page]['contributors']:
                                    editor_text = editor['name']
                                    editor_id = editor['userid']
                                    contributors.add(editor_text)

                    except KeyError:
                        if self.catch_error_to_sleep(response):
                            continue
                        else:
                            break  # TODO: not entirely sure about this terminal condition
                    except requests.exceptions.ConnectionError:
                        print("Max retries exceeded with url.")
                        sleep(5)
                        continue
                    except:
                        print("Throwing except: {}".format(response))
                        continue

                cnt_page, str_pages = 0, ""
                page_set.clear()
        # TODO: check the edits of page contributors, screen out editors who made less than 100 edits
        return contributors

    def search_project_pages(self, search_name):
        psoffset = "0"
        list_project_pages = []

        continue_querying = True
        while continue_querying:
            try:
                query = self.url_propages + "pslimit=" + str(
                    self.const_max_requests) + "&psnamespace=4|5&psoffset=" + str(psoffset) + "&pssearch=" + search_name
                response = requests.get(query).json()
                if 'continue' in response:
                    psoffset = response['continue']['psoffset']
                else:
                    continue_querying = False

                for page in response['query']['prefixsearch']:
                    page_id = page['pageid']
                    page_ns = page['ns']
                    page_title = page['title']

                    self.dict_page_title_id[page_title] = page_id
                    list_project_pages.append(page_title)

            except KeyError:
                if self.catch_error_to_sleep(response):
                    continue
                else:
                    break
            except requests.exceptions.ConnectionError:
                print("Max retries exceeded with url.")
                sleep(5)
                continue
            except:
                print("Throwing except: {}".format(response))
                continue

        return set(list_project_pages)


    def write_rule_recommendations(self):
        # sort the list if needed
        # import operator
        # list_recommended_editors_sorted = sorted(dict_recommended_editors.items(), key=operator.itemgetter(1))

        cwd = os.getcwd()
        fname = cwd + "/data/recommendations/recommendations_rule.csv"

        fout = open(fname, "w")
        print("wikiproject**editor_text**project_edits**wp_edits**last_edit**regstr_time**status", file=fout)
        for wikiproject in self.list_sample_projects:
            for editor_text in self.dict_rule_based_recommendation[wikiproject]:
                print("{}**{}**{}**{}**{}**{}**{}".format(wikiproject, editor_text,
                                                          self.dict_rule_based_recommendation[wikiproject][editor_text],
                                                          self.dict_editor_text_editcount[editor_text],
                                                          self.dict_editor_last_edit_datetime[editor_text],
                                                          self.dict_editor_regstr_time[editor_text],
                                                          self.dict_editor_status[editor_text]), file=fout)

    def write_bonds_recommendations(self):
        cwd = os.getcwd()
        fname = cwd + "/data/recommendations/recommendations_bonds.csv"

        fout = open(fname, "w")
        print("wikiproject**editor_text**pjtk_cnt**talker_cnt**wp_edits**last_edit**regstr_time**status", file=fout)
        for wikiproject in self.list_sample_projects:
            for editor_text in self.dict_bonds_based_recommendation[wikiproject]:
                print("{}**{}**{}**{}**{}**{}**{}**{}".format(wikiproject, editor_text,
                                                              self.dict_bonds_based_recommendation[wikiproject][editor_text],
                                                              self.dict_editor_project_talker_nbr[editor_text][wikiproject],
                                                              self.dict_editor_text_editcount[editor_text],
                                                              self.dict_editor_last_edit_datetime[editor_text],
                                                              self.dict_editor_regstr_time[editor_text],
                                                              self.dict_editor_status[editor_text]), file=fout)

    def write_topic_recommendations(self):
        cwd = os.getcwd()
        fname = cwd + "/data/recommendations/recommendations_topics.csv"
        fout = open(fname, "w")
        print("wikiproject**editor_text**cate_first**cate_second**wp_edits**last_edit**regstr_time**status", file=fout)
        for wikiproject in self.list_sample_projects:
            for editor_text in self.dict_topic_based_recommendation[wikiproject]:
                print("{}**{}**{}**{}**{}**{}**{}**{}".format(wikiproject, editor_text,
                                                              self.dict_topic_editor_first_category[editor_text],
                                                              self.dict_topic_editor_second_category[editor_text],
                                                              self.dict_editor_text_editcount[editor_text],
                                                              self.dict_editor_last_edit_datetime[editor_text],
                                                              self.dict_editor_regstr_time[editor_text],
                                                              self.dict_editor_status[editor_text]), file=fout)

    def write_uucf_recommendations(self):
        cwd = os.getcwd()
        fname = cwd + "/data/recommendations/recommendations_uucf.csv"
        fout = open(fname, "w")
        # experienced_editor,wikiproject,uucf_score,pos,project_member,common_edits
        print("wikiproject**editor_text**project_member**uucf_score**common_edits**wp_edits**last_edit**regstr_time**status", file=fout)
        for wikiproject in self.list_sample_projects:
            for editor_text in self.dict_topic_based_recommendation[wikiproject]:
                print("{}**{}**{}**{}**{}**{}**{}**{}**{}".format(wikiproject, editor_text,
                                                                  self.dict_uucf_similar_project_member[wikiproject][editor_text],
                                                                  self.dict_uucf_based_recommendation[wikiproject][editor_text],
                                                                  self.dict_uucf_common_edits[wikiproject][editor_text],
                                                                  self.dict_editor_text_editcount[editor_text],
                                                                  self.dict_editor_last_edit_datetime[editor_text],
                                                                  self.dict_editor_regstr_time[editor_text],
                                                                  self.dict_editor_status[editor_text]), file=fout)

    def write_newcomer_recommendations(self):
        cwd = os.getcwd()
        fname = cwd + "/data/recommendations/recommendations_newcomers.csv"
        fout = open(fname, "w")
        print("wikiproject**user_text**first_article**project_edits**wp_edits**last_edit**regstr_time**status",
              file=fout)
        for wikiproject in self.dict_project_newcomer_edits.keys():
            for editor_text in self.dict_project_newcomer_edits[wikiproject]:
                print("{}**{}**{}**{}**{}**{}**{}**{}".format(wikiproject, editor_text,
                                                                  self.dict_newcomer_first_edit_article[editor_text],
                                                                  self.dict_project_newcomer_edits[wikiproject][editor_text],
                                                                  self.dict_newcomer_editcount[editor_text],
                                                                  self.dict_editor_last_edit_datetime[editor_text],
                                                                  self.dict_editor_regstr_time[editor_text],
                                                                  "New"), file=fout)

    def read_experienced_project_contributors(self):

        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/project_experienced_contributors.csv"
        print("### Reading members of WikiProjects from file ###")
        header = True
        for line in open(fname, 'r'):
            if header:
                header = False
                continue
            project = line.split('*')[0].strip()
            contributor = line.split('*')[1].strip()
            if project in self.dict_project_contributors:
                self.dict_project_contributors[project].append(contributor)
            else:
                self.dict_project_contributors[project] = [contributor]
        for project in self.dict_project_contributors.keys():
            print("Read {} contributors for WikiProject: {}.".format(len(self.dict_project_contributors[project]),
                                                                        project))

    @staticmethod
    def compute_topic_cosine_similarity(editor_vector, project_vector):
        cos_val = 0
        for cate in project_vector:
            if cate in editor_vector:
                cos_val += project_vector[cate] * editor_vector[cate]
        return cos_val

    @staticmethod
    def read_project_categories():
        print("### Reading categories of the projects from file... ###")
        projects_categories = {}
        filename = os.getcwd() + "/data/pre-processing/projects_categories.csv"
        if os.path.isfile(filename):
            with open(filename, 'r') as fin:
                next(fin)
                for line in fin:
                    project = line.split(",")[0]
                    projects_categories[project] = {}
                    projects_categories[project]['arts'] = float(line.split(",")[1])
                    projects_categories[project]['geography'] = float(line.split(",")[2])
                    projects_categories[project]['health'] = float(line.split(",")[3])
                    projects_categories[project]['mathematics'] = float(line.split(",")[4])
                    projects_categories[project]['history'] = float(line.split(",")[5])
                    projects_categories[project]['science'] = float(line.split(",")[6])
                    projects_categories[project]['people'] = float(line.split(",")[7])
                    projects_categories[project]['philosophy'] = float(line.split(",")[8])
                    projects_categories[project]['religion'] = float(line.split(",")[9])
                    projects_categories[project]['society'] = float(line.split(",")[10])
                    projects_categories[project]['technology'] = float(line.split(",")[11])
                    projects_categories[project]['NF'] = float(line.split(",")[12].strip())

            # TODO: hard coded it for now.. need to change it later...
            projects_categories['Women in Red'] = {}
            projects_categories['Women in Red']['arts'] = 1.0*(projects_categories['Women\'s History']['arts'] + projects_categories['Women artists']['arts'])/2
            projects_categories['Women in Red']['geography'] = 1.0*(projects_categories['Women\'s History']['geography'] + projects_categories['Women artists']['geography'])/2
            projects_categories['Women in Red']['health'] = 1.0*(projects_categories['Women\'s History']['health'] + projects_categories['Women artists']['health'])/2
            projects_categories['Women in Red']['mathematics'] = 1.0*(projects_categories['Women\'s History']['mathematics'] + projects_categories['Women artists']['mathematics'])/2
            projects_categories['Women in Red']['history'] = 1.0*(projects_categories['Women\'s History']['history'] + projects_categories['Women artists']['history'])/2
            projects_categories['Women in Red']['science'] = 1.0*(projects_categories['Women\'s History']['science'] + projects_categories['Women artists']['science'])/2
            projects_categories['Women in Red']['people'] = 1.0*(projects_categories['Women\'s History']['people'] + projects_categories['Women artists']['people'])/2
            projects_categories['Women in Red']['philosophy'] = 1.0*(projects_categories['Women\'s History']['philosophy'] + projects_categories['Women artists']['philosophy'])/2
            projects_categories['Women in Red']['religion'] = 1.0*(projects_categories['Women\'s History']['religion'] + projects_categories['Women artists']['religion'])/2
            projects_categories['Women in Red']['society'] = 1.0*(projects_categories['Women\'s History']['society'] + projects_categories['Women artists']['society'])/2
            projects_categories['Women in Red']['technology'] = 1.0*(projects_categories['Women\'s History']['technology'] + projects_categories['Women artists']['technology'])/2
            projects_categories['Women in Red']['NF'] = 1.0*(projects_categories['Women\'s History']['NF'] + projects_categories['Women artists']['NF'])/2
        return projects_categories


    @staticmethod
    def read_article_projects():
        print("### Reading projects of the articles from file... ###")
        filename = os.getcwd() + "/data/pre-processing/articles_top25_projects.csv"
        article_projects = {}
        if os.path.isfile(filename):
            for line in open(filename, 'r'):
                article = line.split(",")[1]
                project = line.split(",")[2].strip()
                # Hard code for project "Women in Red"
                if project in ['Women', 'Women artists', 'Women\'s History', 'Women scientists']:
                    project = 'Women in Red'

                if article in article_projects:
                    article_projects[article].append(project)
                else:
                    article_projects[article] = [project]

        return article_projects

    @staticmethod
    def read_article_categories():

        print("### Reading categories of the articles from file... ###")
        filename = os.getcwd() + "/data/pre-processing/articles_categories.csv"

        article_categories = {}
        debug = True
        if os.path.isfile(filename):
            with open(filename, 'r') as fin:
                next(fin)
                for line in fin:
                    try:
                        article = line.split(",")[0]
                        category = line.split(",")[1]
                        cnt = int(line.split(",")[2].strip())
                    except ValueError:
                        # skip file reading errors ..
                        continue

                    if article not in article_categories:
                        article_categories[article] = {}
                        article_categories[article][category] = cnt
                    else:
                        article_categories[article][category] = cnt
        return article_categories

    @staticmethod
    def catch_error_to_sleep(response):
        if "error" in response:
            print("Code: {}; Info {}".format(response['error']['code'],
                                             response['error']['info']))

            if response['error']['code'] == 'maxlag':
                ptime = max(5, int(response.headers['Retry-After']))
                print('WD API is lagged, waiting {} seconds to try again'.format(ptime))
                sleep(ptime)
                return True

            if response['error']['code'] == 'internal_api_error_DBQueryError':
                sleep(5)
                return True

        return False

    @staticmethod
    def read_active_editors():
        print("### Reading active editors in the last week from the file... ###")
        filename = "data/pre-processing/active_editors_past_week.csv"
        list_editors = []
        # each line only contain an editor name
        for line in open(filename, "r").readlines()[1:]:
            list_editors.append(line.replace("\n", ""))
        return list_editors

    @staticmethod
    def read_sample_projects():
        # each line only contain an editor name
        filename = "data/pre-processing/Top25ProjectsPastYear.csv"
        list_projects = []
        dict_project_identity_based_recommendation = {}
        dict_project_rule_based_recommendation = {}
        dict_project_bonds_based_recommendation = {}
        dict_project_newcomer_edits = {}
        dict_project_uucf_based_recommendation = {}

        dict_editor_similar_project_member = {}
        dict_uucf_common_edits = {}

        for line in open(filename, "r").readlines()[1:]:
            project = line.split(",")[0].replace("WikiProject_", "").replace("_", " ")
            list_projects.append(project)
            dict_project_rule_based_recommendation[project] = {}
            dict_project_bonds_based_recommendation[project] = {}
            dict_project_newcomer_edits[project] = {}
            dict_project_identity_based_recommendation[project] = {}
            dict_project_uucf_based_recommendation[project] = {}

            dict_editor_similar_project_member[project] = {}
            dict_uucf_common_edits[project] = {}

        return list_projects, dict_project_rule_based_recommendation, \
               dict_project_identity_based_recommendation, dict_project_bonds_based_recommendation, \
               dict_project_uucf_based_recommendation, dict_project_newcomer_edits, dict_editor_similar_project_member, \
               dict_uucf_common_edits

    def write_valid_project_contributors_to_file(self):
        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/project_experienced_contributors.csv"
        fout = open(fname, "w")

        with open(fname, "w") as fout:
            print("wikiproject,contributor", file=fout)
            for project in self.dict_project_contributors:
                for contributor in self.dict_project_contributors[project]:
                    print("{}*{}".format(project, contributor), file=fout)

    def write_valid_experienced_editors_to_file(self):
        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/valid_experienced_editors.csv"
        fout = open(fname, "w")

        with open(fname, "w") as fout:
            print("user_text,user_id,editcount,regtr_time", file=fout)
            for user_text in self.set_valid_exp_editors:
                print("{}*{}*{}*{}".format(user_text,
                                           self.dict_exp_editor_text_id[user_text],
                                           self.dict_editor_text_editcount[user_text],
                                           self.dict_editor_regstr_time[user_text]), file=fout)

    def write_valid_newcomers_to_file(self):
        cwd = os.getcwd()
        fname = cwd + "/data/pre-processing/valid_newcomers.csv"
        fout = open(fname, "w")
        with open(fname, "w") as fout:
            print("newcomer,user_id,editcount,regtr_time", file=fout)
            for user_text in self.set_valid_newcomers:
                print("{}*{}*{}*{}".format(user_text,
                                           self.dict_newcomer_text_id[user_text],
                                           self.dict_newcomer_editcount[user_text],
                                           self.dict_editor_regstr_time[user_text]), file=fout)

    def read_valid_experienced_editors_from_file(self):
        print("### Reading valid experienced editors from the file... ###")
        fname = os.getcwd() + '/data/pre-processing/valid_experienced_editors.csv'
        header = True
        for line in open(fname, 'r'):
            if header:
                header = False
                continue
            user_text = line.split('*')[0]
            self.dict_exp_editor_text_id[user_text] = line.split('*')[1]
            if line.split('*')[2] == '':
                continue
            self.dict_editor_text_editcount[user_text] = int(line.split('*')[2])
            self.dict_editor_regstr_time[user_text] = line.split('*')[3].strip()

            self.set_valid_exp_editors.add(user_text)
        print("Read {} valid experienced editors from file".format(len(self.set_valid_exp_editors)))

    def read_valid_newcomer_from_file(self):
        print("### Reading valid newcomers from the file... ###")
        fname = os.getcwd() + '/data/pre-processing/valid_newcomers.csv'
        header = True
        for line in open(fname, 'r'):
            if header:
                header = False
                continue
            user_text = line.split('*')[0]
            self.dict_newcomer_text_id[user_text] = line.split('*')[1]
            if line.split('*')[2] == '':
                continue
            self.dict_newcomer_editcount[user_text] = int(line.split('*')[2])
            self.dict_editor_regstr_time[user_text] = line.split('*')[3].strip()

            self.set_valid_newcomers.add(user_text)
        print("Read {} valid newcomers from file".format(len(self.set_valid_newcomers)))

    @staticmethod
    def read_bot_list():
        filename = os.getcwd() + "/data/bot_list.csv"
        # each line only contain an editor name
        list_bot = set()
        for line in open(filename, "r").readlines()[1:]:
            list_bot.add(line.strip())
        return list_bot


    def recommend_editors_WIR(self):

        if not self.dict_project_contributors:
            self.read_experienced_project_contributors()

        print("### Recommending editors for WIR... ###")
        list_editor_sorted, dict_editor_page_creation, dict_editor_article = self.page_parser.identify_WIR_article_creators()
        cnt_editor, str_editors, cnt_recommendations = 0, "", 0
        editors_WIR = {}

        for i in range(len(list_editor_sorted)):
            editor_text = list_editor_sorted[i]

            if cnt_recommendations >= self.const_recommendation_nbr:
                break

            if editor_text in self.dict_project_contributors['Women in Red']:
                continue

            # TODO: this cause problem for the last batch of editors - they are not counted
            if cnt_editor < 45 and i != len(list_editor_sorted)-1:
                cnt_editor += 1
                str_editors += editor_text + "|"
            else:
                try:
                    str_editors += editor_text + "|"
                    query = self.url_userinfo + "&usprop=editcount|registration&ususers=" + str_editors
                    response = requests.get(query).json()
                    for editor_info in response['query']['users']:

                        if cnt_recommendations >= self.const_recommendation_nbr:
                            break

                        if 'userid' not in editor_info:
                            continue

                        # check basic info
                        editor_text = editor_info['name']
                        editor_id = editor_info['userid']
                        editor_editcount = editor_info['editcount']
                        editor_regstr_ts = editor_info['registration']

                        if editor_regstr_ts is None:
                            continue
                        regstr_datetime = datetime.strptime(editor_regstr_ts, "%Y-%m-%dT%H:%M:%SZ")

                        # check most recent 500 edits
                        query = self.url_usercontb + "uclimit=" + str(self.const_max_requests) + "&ucnamespace=0|3|4|5&" \
                                                                                                 "ucprop=title|timestamp|parsedcomment|flags|ids&ucuser=" + editor_text
                        response = requests.get(query).json()
                        latest_datetime = datetime.fromordinal(1)
                        current_datetime = datetime.now()
                        cnt_mainpage_edits = 0

                        for usercontrib in response['query']['usercontribs']:
                            ns = usercontrib['ns']

                            edit_datetime = datetime.strptime(usercontrib['timestamp'], "%Y-%m-%dT%H:%M:%SZ")
                            latest_datetime = max(edit_datetime, latest_datetime)
                            self.dict_editor_last_edit_datetime[editor_text] = latest_datetime

                            if 'parsedcomment' in usercontrib:
                                comment = usercontrib['parsedcomment']
                                if comment.startswith('Revert'):
                                    continue

                            if ns == 0:
                                # count the number of edits within 30 days
                                if (current_datetime - edit_datetime).days <= self.const_one_month_in_days:
                                    cnt_mainpage_edits += 1


                        if cnt_mainpage_edits >= self.threshold_very_active:
                            status = self.const_very_active
                        elif cnt_mainpage_edits >= self.threshold_active:
                            status = self.const_active
                        else:
                            continue

                        editors_WIR[editor_text] = {"page_created": dict_editor_page_creation[editor_text],
                                                    "editor_editcount": editor_editcount,
                                                    "editor_regstr_ts": editor_regstr_ts,
                                                    "sample_article": dict_editor_article[editor_text],
                                                    "status": status}
                        cnt_recommendations += 1

                except KeyError:
                    if self.catch_error_to_sleep(response):
                        continue
                    else:
                        break
                except requests.exceptions.ConnectionError:
                    print("Max retries exceeded with url.")
                    sleep(5)
                    continue
                except:
                    print("Throwing except: {}".format(response))
                    continue

                cnt_editor, str_editors = 0, ""

        cwd = os.getcwd()
        fout = open(cwd + "/data/recommendations/recommendations_WIR.csv", "w")
        print("editor_text,page_created,editcount,regstr_ts,article,status", file=fout)
        for editor_text in editors_WIR:
            print("{}**{}**{}**{}**{}**{}".format(editor_text,
                                                 editors_WIR[editor_text]['page_created'],
                                                 editors_WIR[editor_text]['editor_editcount'],
                                                 editors_WIR[editor_text]['editor_regstr_ts'],
                                                 editors_WIR[editor_text]['sample_article'],
                                                 editors_WIR[editor_text]['status']), file=fout)





    def execute(self):

        # pre-process
        self.collect_project_related_pages()
        self.identify_project_members_and_edits()

        self.identify_valid_newcomers_and_experienced_editors()

        self.recommend_editors()

        # recommend uu cf based editors
        # self.uucf.recommend_editors()




def main():
    from sys import argv
    # if len(argv) != 2:
    # print("Usage: <active_editors> <sample_projects> <bot_list> <article_projects>")
    # return

    rec_exp = RecommendExperienced(argv)

    rec_exp.execute()
    rec_exp.recommend_editors_WIR()

    table_generator = TableGenerator(rec_exp.dict_editor_text_editcount, rec_exp.dict_editor_regstr_time, rec_exp.dict_editor_status)
    table_generator.create_tables()



import time

start_time = time.time()
main()
print("--- {} hours ---".format(1.0 * (time.time() - start_time) / 3600))